{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "final_refined = pd.read_pickle(\"final_refined.pkl\")  ##lets load the cleaned data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\AAn...</td>\n",
       "      <td>b webmaster freelance siti web trapani vorrebb...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\ACa...</td>\n",
       "      <td>rt modena apolitiche aprono campagna elettoral...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\ACa...</td>\n",
       "      <td>b capiscono colpo stato potrebbero pi xc3 xb9 ...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\ACi...</td>\n",
       "      <td>b lobotomizzato analfabeta confrontarsi libran...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\AEr...</td>\n",
       "      <td>rt cosa devono dire capitali europee consegnat...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  \\\n",
       "0  C:/Users/Hp/Desktop/Final project/RightNew\\AAn...   \n",
       "1  C:/Users/Hp/Desktop/Final project/RightNew\\ACa...   \n",
       "2  C:/Users/Hp/Desktop/Final project/RightNew\\ACa...   \n",
       "3  C:/Users/Hp/Desktop/Final project/RightNew\\ACi...   \n",
       "4  C:/Users/Hp/Desktop/Final project/RightNew\\AEr...   \n",
       "\n",
       "                                              tweets  label  \n",
       "0  b webmaster freelance siti web trapani vorrebb...  Right  \n",
       "1  rt modena apolitiche aprono campagna elettoral...  Right  \n",
       "2  b capiscono colpo stato potrebbero pi xc3 xb9 ...  Right  \n",
       "3  b lobotomizzato analfabeta confrontarsi libran...  Right  \n",
       "4  rt cosa devono dire capitali europee consegnat...  Right  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_refined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I relized there are some English words so I removed the stop words \n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.lower()\n",
    "    words = words.split()\n",
    "    noise_free_words = [word for word in words if word not in stop_words] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"tweets\"] = final_refined[\"tweets\"].apply(lambda s: _remove_noise(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    rt modena apolitiche aprono campagna elettoral...\n",
       "1    b thanking god life well spent burial ceremony...\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['tweets'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "##lets reduce the number of words and proceed to feature selection\n",
    "##lets break the string of text into words-Tokenizing\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\AAn...</td>\n",
       "      <td>[b, webmaster, freelance, siti, web, trapani, ...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\ACa...</td>\n",
       "      <td>[rt, modena, apolitiche, aprono, campagna, ele...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\ACa...</td>\n",
       "      <td>[b, capiscono, colpo, stato, potrebbero, pi, x...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\ACi...</td>\n",
       "      <td>[b, lobotomizzato, analfabeta, confrontarsi, l...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\AEr...</td>\n",
       "      <td>[rt, cosa, devono, dire, capitali, europee, co...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\AJD...</td>\n",
       "      <td>[rt, good, morning, freinds, r, ciao, rt, henr...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\ALV...</td>\n",
       "      <td>[rt, rt, hope, get, 750, 000, followers, 75th,...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\ALi...</td>\n",
       "      <td>[b, atletico, nper, poco, juve, narriva, pareg...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\AM_...</td>\n",
       "      <td>[rt, brave, ragazze, bisogna, sputtanare, gent...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\ANT...</td>\n",
       "      <td>[b, ancora, berlusca, ahahahahahha, b, aspetta...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\AOC...</td>\n",
       "      <td>[rt, spinato, live, continua, guerrilla, marke...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\APa...</td>\n",
       "      <td>[direttivo, inter, club, cucchi, terranova, si...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\APu...</td>\n",
       "      <td>[rt, ordini, commesse, liquidazione, giallo, r...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\ASR...</td>\n",
       "      <td>[rt, buone, notizie, cittadini, ambiente, rt, ...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\Aa5...</td>\n",
       "      <td>[richiede, nuovi, tiwter, fa, prima, rt, oggi,...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\Aar...</td>\n",
       "      <td>[coglione, daniela, culo, proprio, cojone, sai...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\Aar...</td>\n",
       "      <td>[6, numero, preferito, secondo, 06, mette, sec...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\Abi...</td>\n",
       "      <td>[b, rt, vicenda, xe2, x80, x99entra, fondazion...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\Adr...</td>\n",
       "      <td>[amalia, amalia, mohamedd, ali, vado, errando,...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C:/Users/Hp/Desktop/Final project/RightNew\\Adr...</td>\n",
       "      <td>[glues, avenue, nascono, gennaio, 2018, band, ...</td>\n",
       "      <td>Right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename  \\\n",
       "0   C:/Users/Hp/Desktop/Final project/RightNew\\AAn...   \n",
       "1   C:/Users/Hp/Desktop/Final project/RightNew\\ACa...   \n",
       "2   C:/Users/Hp/Desktop/Final project/RightNew\\ACa...   \n",
       "3   C:/Users/Hp/Desktop/Final project/RightNew\\ACi...   \n",
       "4   C:/Users/Hp/Desktop/Final project/RightNew\\AEr...   \n",
       "5   C:/Users/Hp/Desktop/Final project/RightNew\\AJD...   \n",
       "6   C:/Users/Hp/Desktop/Final project/RightNew\\ALV...   \n",
       "7   C:/Users/Hp/Desktop/Final project/RightNew\\ALi...   \n",
       "8   C:/Users/Hp/Desktop/Final project/RightNew\\AM_...   \n",
       "9   C:/Users/Hp/Desktop/Final project/RightNew\\ANT...   \n",
       "10  C:/Users/Hp/Desktop/Final project/RightNew\\AOC...   \n",
       "11  C:/Users/Hp/Desktop/Final project/RightNew\\APa...   \n",
       "12  C:/Users/Hp/Desktop/Final project/RightNew\\APu...   \n",
       "13  C:/Users/Hp/Desktop/Final project/RightNew\\ASR...   \n",
       "14  C:/Users/Hp/Desktop/Final project/RightNew\\Aa5...   \n",
       "15  C:/Users/Hp/Desktop/Final project/RightNew\\Aar...   \n",
       "16  C:/Users/Hp/Desktop/Final project/RightNew\\Aar...   \n",
       "17  C:/Users/Hp/Desktop/Final project/RightNew\\Abi...   \n",
       "18  C:/Users/Hp/Desktop/Final project/RightNew\\Adr...   \n",
       "19  C:/Users/Hp/Desktop/Final project/RightNew\\Adr...   \n",
       "\n",
       "                                               tweets  label  \n",
       "0   [b, webmaster, freelance, siti, web, trapani, ...  Right  \n",
       "1   [rt, modena, apolitiche, aprono, campagna, ele...  Right  \n",
       "2   [b, capiscono, colpo, stato, potrebbero, pi, x...  Right  \n",
       "3   [b, lobotomizzato, analfabeta, confrontarsi, l...  Right  \n",
       "4   [rt, cosa, devono, dire, capitali, europee, co...  Right  \n",
       "5   [rt, good, morning, freinds, r, ciao, rt, henr...  Right  \n",
       "6   [rt, rt, hope, get, 750, 000, followers, 75th,...  Right  \n",
       "7   [b, atletico, nper, poco, juve, narriva, pareg...  Right  \n",
       "8   [rt, brave, ragazze, bisogna, sputtanare, gent...  Right  \n",
       "9   [b, ancora, berlusca, ahahahahahha, b, aspetta...  Right  \n",
       "10  [rt, spinato, live, continua, guerrilla, marke...  Right  \n",
       "11  [direttivo, inter, club, cucchi, terranova, si...  Right  \n",
       "12  [rt, ordini, commesse, liquidazione, giallo, r...  Right  \n",
       "13  [rt, buone, notizie, cittadini, ambiente, rt, ...  Right  \n",
       "14  [richiede, nuovi, tiwter, fa, prima, rt, oggi,...  Right  \n",
       "15  [coglione, daniela, culo, proprio, cojone, sai...  Right  \n",
       "16  [6, numero, preferito, secondo, 06, mette, sec...  Right  \n",
       "17  [b, rt, vicenda, xe2, x80, x99entra, fondazion...  Right  \n",
       "18  [amalia, amalia, mohamedd, ali, vado, errando,...  Right  \n",
       "19  [glues, avenue, nascono, gennaio, 2018, band, ...  Right  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['tweets'] = final_df['tweets'].apply(lambda s: tokenizer.tokenize(s))\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "##let us lematize the words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = [lemmatizer.lemmatize(i) for i in text]\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [b, webmaster, freelance, siti, web, trapani, ...\n",
       "1      [rt, modena, apolitiche, aprono, campagna, ele...\n",
       "2      [b, capiscono, colpo, stato, potrebbero, pi, x...\n",
       "3      [b, lobotomizzato, analfabeta, confrontarsi, l...\n",
       "4      [rt, cosa, devono, dire, capitali, europee, co...\n",
       "                             ...                        \n",
       "895    [rilassati, eccolo, qua, larena, guarda, scopa...\n",
       "896    [b, rt, amiamo, sagre, xc3, xa8, pi, xc3, xb9,...\n",
       "897    [rt, lilli, gruber, detesta, testosterone, per...\n",
       "898    [spadafora, serio, problema, razzismo, stadi, ...\n",
       "899    [simpatica, vigliero, tombare, rom, rispetto, ...\n",
       "Name: tweets, Length: 1797, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['tweets'].apply(lambda x: word_lemmatizer(x))  ##this takes alot of time to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##lets stematize the words further\n",
    "stemmer=SnowballStemmer(\"italian\")\n",
    "def word_stemmer(text):\n",
    "    stem_text = \" \".join([stemmer.stem(i) for i in text])\n",
    "    return stem_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      b webmaster freelanc sit web trapan vorrebb ri...\n",
       "1      rt moden apolit apron campagn elettoral bonacc...\n",
       "2      b cap colp stat potrebber pi xc3 xb9 esprim li...\n",
       "3      b lobotomizz analfabet confront librand dovreb...\n",
       "4      rt cos dev dir capital europe consegn citt sin...\n",
       "                             ...                        \n",
       "895    rilass eccol qua laren guard scop sol coscenz ...\n",
       "896    b rt amiam sagr xc3 xa8 pi xc3 xb9 dolc ix edi...\n",
       "897    rt lill gruber detest testosteron perc trascur...\n",
       "898    spadafor ser problem razzism stad alban 51 mor...\n",
       "899    simpat viglier tomb rom rispett tre signor vig...\n",
       "Name: tweets, Length: 1797, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['tweets'].apply(lambda x: word_stemmer(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_pickle('final_df.pkl')  ##save the stemmed and lematized data to pickle so that I load it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_pickle('final_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##we need to transform the lemmas to a string before applying TFIDF\n",
    "##the words are in a list and hence we need to join them so that we transform using TFIDF\n",
    "final_df_transform = final_df['tweets'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     b webmaster freelance siti web trapani vorrebb...\n",
       "1     rt modena apolitiche aprono campagna elettoral...\n",
       "2     b capiscono colpo stato potrebbero pi xc3 xb9 ...\n",
       "3     b lobotomizzato analfabeta confrontarsi libran...\n",
       "4     rt cosa devono dire capitali europee consegnat...\n",
       "5     rt good morning freinds r ciao rt henri rousse...\n",
       "6     rt rt hope get 750 000 followers 75th annivers...\n",
       "7     b atletico nper poco juve narriva pareggio b f...\n",
       "8     rt brave ragazze bisogna sputtanare gente sopr...\n",
       "9     b ancora berlusca ahahahahahha b aspettando ca...\n",
       "10    rt spinato live continua guerrilla marketing i...\n",
       "11    direttivo inter club cucchi terranova sibari c...\n",
       "12    rt ordini commesse liquidazione giallo rt demo...\n",
       "13    rt buone notizie cittadini ambiente rt grazie ...\n",
       "14    richiede nuovi tiwter fa prima rt oggi festegg...\n",
       "15    coglione daniela culo proprio cojone sai paghi...\n",
       "16    6 numero preferito secondo 06 mette secondo de...\n",
       "17    b rt vicenda xe2 x80 x99entra fondazione nulla...\n",
       "18    amalia amalia mohamedd ali vado errando qui tr...\n",
       "19    glues avenue nascono gennaio 2018 band compost...\n",
       "Name: tweets, dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_transform.head(20) ##the joined data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "##transform the data using TFIDF\n",
    "X = final_df_transform\n",
    "y = final_df.label\n",
    "features_train, features_test,labels_train,labels_test = train_test_split(X,y, test_size = 0.2, random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5)\n",
    "##transform the train and text\n",
    "features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "features_test_transformed = vectorizer.fit_transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1437, 415093)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##I realized you have to transform the whole of X to get rid of the shape error\n",
    "##transform X using TFIDF\n",
    "X_transformed = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed.shape\n",
    "#(1797, 466747)  ##we have 466,747 features. We need to get the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectPercentile(percentile=10,\n",
       "                 score_func=<function f_classif at 0x00000213439AFE58>)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectPercentile(f_classif, percentile = 10)  ##here we get the 10%best features\n",
    "selector.fit(X_transformed, y)\n",
    "\n",
    "#features_train_transformed = selector.transform(features_test_transformed).toarray()\n",
    "#features_test_transformed = selector.transform(features_test_transformed).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_best = selector.transform(X_transformed).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 46675)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_best.shape ##this are our best features 46,675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###An alternative of feature selection is select K best\n",
    "#apply select K best\n",
    "##this dint work best for my model so I used the 10 percentile feature selection\n",
    "best_features = SelectKBest(score_func=f_regression,k=10).fit_transform(X_transformed,y)\n",
    "#fit = best_features(X_transformed,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##import all the packages\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7037037037037037\n"
     ]
    }
   ],
   "source": [
    "##Grid-search is used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions.\n",
    "#lets do some gridsearch on the data\n",
    "#I had already built some dummy classifiers to check the overall accuracy of my data which was at an average of 48%\n",
    "\n",
    "##let us now train the model\n",
    "X_best_train, X_best_test,labels_train,labels_test = train_test_split(X_best,y, test_size = 0.3, random_state = 40)\n",
    "nb = MultinomialNB()\n",
    "              \n",
    "nb.fit(X_best_train, labels_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_best_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, labels_test))\n",
    "##the accuracy for Naive Bayes is 70% which is quite good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9111111111111111\n"
     ]
    }
   ],
   "source": [
    "##using SGD classifier which is best for text classification\n",
    "sgd =  SGDClassifier(loss='hinge', penalty='l2')\n",
    "\n",
    "sgd.fit(X_best_train, labels_train)\n",
    "\n",
    "y_pred = sgd.predict(X_best_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, labels_test))\n",
    "##the accuracy is 91 % which is impressive\n",
    "##you ca try other models and determine the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "##transform x and y using count vectorizer and TFIDF\n",
    "X_transformed = vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 466747)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##the below was the performance using Select k = 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1797x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 46 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##We have reduced from 466747 features to 50 features\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "##let us now train the model\n",
    "best_features_train, best_features_test,labels_train,labels_test = train_test_split(best_features,y, test_size = 0.2, random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.46944444444444444\n"
     ]
    }
   ],
   "source": [
    "##applying Naive Bayes\n",
    "nb = MultinomialNB()\n",
    "              \n",
    "nb.fit(best_features_train, labels_train)\n",
    "#%%time\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(best_features_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd =  SGDClassifier(loss='hinge', penalty='l2')\n",
    "\n",
    "sgd.fit(best_features_train,labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.4638888888888889\n",
      "Wall time: 3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_pred = sgd.predict(best_features_test,)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##using gridsearch\n",
    "##this took alot of time \n",
    "#parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "#              'tfidf__use_idf': (True, False),\n",
    "#              'clf__alpha': (1e-2, 1e-3), }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gs_clf = GridSearchCV(nb, parameters, n_jobs=-1)\n",
    "#gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##lets now test the model SGD\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
